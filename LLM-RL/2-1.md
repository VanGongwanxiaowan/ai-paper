## 第一章：深度强化学习基础——马尔可夫决策过程（MDP）

### 课程目标与范围
- **核心目标**：介绍与大型语言模型强化学习及时序缩放直接相关的深度强化学习内容，而非涵盖现代深度强化学习的全部领域。
- **背景假设**：学习者需熟悉神经网络与反向传播，但**不要求**事先了解强化学习。

---

### 1. 强化学习与监督学习的核心区别
- **强化学习（RL）**：专注于**序列决策**问题，智能体需在时间线上连续做出多个决策以实现特定目标。
- **监督学习（Supervised Learning）**：通常处理**单次决策**问题（如图像分类、垃圾邮件识别）。

#### 强化学习典型应用场景：
- **博弈类**：国际象棋、围棋（需连续行棋）。
- **控制类**：自动驾驶车辆（需连续控制刹车、加速、转向）。
- **其他**：视频游戏操作、机器人控制等。

---

### 2. 马尔可夫决策过程（MDP）基础框架

#### 2.1 核心概念
- **环境模型**：智能体交互的环境形式化为一个 **MDP**。
- **时间表示**：
    - 时间通常是**离散的**（记为 \( t = 0, 1, 2, \dots, T \)）。
    - 对于大多数与大型语言模型相关的应用，离散时间假设已足够。
    - 某些物理系统（如火箭控制）本质是连续时间，但常被离散化处理。
- **终止时间** \( T \)：
    - 当达到**终止状态**时，回合结束。
    - 若永远无法达到终止状态，则 \( T = \infty \)。

#### 2.2 MDP 组成要素
1. **状态（State）**：
    - 当前时刻 \( t \) 的状态记为 \( s_t \)。
    - 状态空间 \( \mathcal{S} \) 是非空集合。
    - \( \mathcal{S}^+ \)：包含所有非终止状态与终止状态的集合。

2. **动作（Action）**：
    - 在状态 \( s_t \) 下，智能体根据**策略（Policy）** \( \pi \) 选择动作 \( a_t \)。
    - 动作空间 \( \mathcal{A} \) 是非空集合。

3. **奖励（Reward）**：
    - 执行动作 \( a_t \) 后，环境反馈奖励 \( r_t \)。
    - 奖励为**标量实数**（非向量），且取值有限（排除 ±∞）。

4. **状态转移（Transition）**：
    - 由环境动力学决定，概率表示为 \( p(s_{t+1}, r_t | s_t, a_t) \)。
    - 智能体通常**不知道**确切的转移概率，需通过与环境的交互来学习。

5. **初始状态**：
    - 初始状态 \( s_0 \) 从初始分布 \( p_0 \) 中采样。
    - 有时是固定的（如视频游戏每次从同一画面开始）。

---

### 3. MDP 的动态过程与轨迹
- **轨迹（Trajectory）** \( \tau \)：
    \[
    \tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T)
    \]
- **单步循环**：
    1. 在状态 \( s_t \)。
    2. 策略 \( \pi \) 选择动作 \( a_t \)。
    3. 环境返回奖励 \( r_t \) 和下一状态 \( s_{t+1} \)。
    4. 重复直至终止状态 \( s_T \)。

---

### 4. 任务分类与终止条件
- **回合式任务（Episodic Task）**：
    - 存在终止状态，\( T \) 为有限值。
    - 示例：一局游戏结束（胜利或失败）。
- **持续式任务（Continuing Task）**：
    - 无终止状态，\( T = \infty \)。
    - 智能体持续交互。

---

### 5. 动力学特性补充说明
- **奖励的确定性**：
    - 有时奖励是状态与动作的确定性函数：\( r_t = f(s_t, a_t) \)。
- **转移的确定性**：
    - 有时下一状态也是确定性的：\( s_{t+1} = g(s_t, a_t) \)（如按下“上”键，角色精确上移一格）。
- **随机性**：
    - 环境通常包含随机性（如打开宝箱获得随机装备）。

---

### 6. 强化学习的核心目标
- 智能体的目标是**最大化累积奖励**（具体形式后续讲解）。
- **关键挑战**：
    - 环境动力学（转移概率）未知。
    - 智能体必须在**探索环境**与**利用已知信息**之间做出权衡，通过交互学习策略。

---

### 总结要点
| 概念 | 说明 |
|------|------|
| **核心区别** | RL 处理**序列决策**，监督学习处理**单次决策**。 |
| **MDP 要素** | 状态 \( s_t \)、动作 \( a_t \)、奖励 \( r_t \)、转移概率 \( p \)。 |
| **策略** | \( \pi \) 决定在给定状态下如何选择动作。 |
| **任务类型** | 回合式（有限步） vs 持续式（无限步）。 |
| **环境未知性** | 转移概率通常未知，需通过交互学习。 |
| **目标** | 学习策略以最大化长期累积奖励。 |

此部分为深度强化学习的基础，为后续理解如何将其应用于大型语言模型的训练与优化奠定了框架。

## 第二章：马尔可夫决策过程（MDP）的细节与扩展

### 1. 动力学特性与策略

#### 1.1 状态转移的确定性与随机性
- **确定性转移**：下一状态 \( s_{t+1} \) 可能是当前状态 \( s_t \) 与动作 \( a_t \) 的确定性函数。
- **随机性转移**：下一状态是一个以当前状态和动作为条件的随机变量。
- **课程假设**：为简化分析，假设动力学是**平稳的（Stationary）**。

#### 1.2 平稳 vs 非平稳动力学
- **非平稳动力学**：
    - 转移概率随时间步 \( t \) 变化，记为 \( P_t \)。
    - **现实场景**：例如，在视频游戏中，早期策略与临近结束时的策略（已知游戏即将结束）通常不同。
- **平稳动力学**：
    - 转移概率与时间步无关，记为 \( P \)（无下标 \( t \)）。
    - 环境交互规则不随时间变化。
- **课程选择**：后续讨论将假设平稳动力学。

#### 1.3 策略的类型与表示
- **策略 \( \pi \)**：智能体根据当前状态选择动作的规则。
- **随机性策略**：
    - \( \pi(a|s) \) 定义了一个概率分布：\( \sum_a \pi(a|s) = 1 \)。
    - 动作根据该分布采样得到：\( a_t \sim \pi(\cdot|s_t) \)。
- **确定性策略**：
    - 直接输出一个特定动作：\( a_t = \pi(s_t) \)。
- **参数化策略**：
    - 策略常由神经网络参数化：\( \pi = \pi_\theta \)，其中 \( \theta \) 为网络参数。

---

### 2. 可观测性问题与POMDP

#### 2.1 完全可观测 vs 部分可观测
- **完全可观测 MDP**：智能体能直接观察到完整的真实状态 \( s_t \)，系统满足马尔可夫性。
- **部分可观测马尔可夫决策过程（POMDP）**：
    - 智能体只能获得**观察（Observation）**，而非完整状态。
    - **示例**：老虎躲在树后，你当前看不到老虎，但你知道它存在。过去的历史（曾看到老虎）对当前决策至关重要。
    - **挑战**：POMDP 比 MDP 更难处理，但在现实世界中非常普遍。

#### 2.2 本课程的假设
- 假设智能体处于**完全可观测**的 MDP 中。
- **理由**：在大型语言模型的强化学习上下文中，模型能观察到完整的对话历史，而对话历史本身就构成了完整状态。
- 因此，无需区分状态与观察，避免了 POMDP 的复杂性。

---

### 3. MDP的其他常见扩展（简要提及）

| 扩展类型 | 说明 | 本课程处理方式 |
|----------|------|----------------|
| **非平稳动力学** | 转移概率 \( P_t \) 随时间变化。 | **假设平稳**。 |
| **预定终止时间** | 终止时间 \( T \) 是预先确定的固定值，而非由到达终止状态触发。 | 不采用，通常与时间相关策略相关。 |
| **时间相关策略** | 策略 \( \pi_t \) 依赖于时间步。 | 不需要，因为动力学平稳。 |
| **状态相关的动作集** | 可执行的动作集合 \( \mathcal{A}(s) \) 依赖于当前状态。 | 为简化，**假设动作集恒定**。 |

- **实践说明**：真实世界的 MDP 往往包含上述一个或多个复杂特性。本课程讲授核心原理，掌握后可自行适配到具体问题。

---

### 4. 终止时间的处理与数学简化

#### 4.1 终止时间是随机变量
- 终止时间 \( T \) 是一个**整数值随机变量**。
- **示例**：玩视频游戏时，游戏结束（赢或输）的时刻取决于你的操作和环境反馈，每次都可能不同。

#### 4.2 数学处理上的难点
- 涉及从当前时间到随机终止时间 \( T \) 的求和时，需要格外小心。
- **关键问题**：**不能随意交换期望和求和的顺序**。
    - 错误示例：\( \mathbb{E}[\sum_{t=0}^{T} r_t] \neq \sum_{t=0}^{T} \mathbb{E}[r_t] \)（因为右侧的 \( T \) 仍在期望外，是随机的，无意义）。
- **原因**：求和上限 \( T \) 本身是随机的，将其移到期望外会留下一个未取期望的随机变量，导致表达式无定义。

#### 4.3 标准简化技巧：引入吸收状态
- **目的**：为了简化理论和记号，将原MDP转换为一个**名义上永不终止**的等价MDP。
- **方法**：
    1. 将原终止状态变为**吸收状态**。
    2. 一旦进入吸收状态，智能体会永远停留在此状态。
    3. 从吸收状态出发，获得的奖励恒为 0（或任意常数值）。
- **效果**：
    - 新MDP的终止时间 \( T = \infty \)（无限时间步）。
    - 求和变为从 0 到 ∞，求和上限不再是随机变量。
    - 此时可以安全地交换期望与求和的顺序：\( \mathbb{E}[\sum_{t=0}^{\infty} r_t] = \sum_{t=0}^{\infty} \mathbb{E}[r_t] \)。
- **实质**：这种转换保持了原MDP在达到终止状态前的所有轨迹和累积奖励不变，只是数学上更易处理。

---

### 总结要点
- **动力学平稳性**是本课程的核心假设，它简化了策略与值函数的形式。
- **完全可观测性**是另一个关键假设，适用于大型语言模型场景（完整对话历史即状态）。
- **策略**可以是随机或确定的，且通常由神经网络参数化。
- **终止时间的随机性**带来了数学复杂性，通过引入**吸收状态**可将其转换为无限时域问题，从而简化分析。
- 真实问题可能更复杂，但掌握基础MDP理论后，可将其原理扩展至非平稳、部分可观测等实际情况。

## 第三章：模仿学习、分布偏移与强化学习范式

### 1. 吸收状态的技术细节
- **目的**：将原MDP转换为一个等价的名义上无限时长的MDP，以简化数学处理。
- **方法**：
    1. 将**终止状态**视为一个特殊的“普通”状态。
    2. 一旦进入终止状态，无论采取任何动作：
        - **状态转移**：以概率 1 停留在该终止状态。
        - **奖励**：获得的奖励恒为 0。
    3. 系统陷入“时间冻结”的循环，永不改变。
- **策略影响**：在终止状态下，策略变得**无关紧要**，因为任何动作都不会改变状态或产生奖励。

---

### 2. 模仿学习（行为克隆）

#### 2.1 基本思想
- **目标**：训练一个参数化策略 \( \pi_\theta \)（通常为神经网络），使其**模仿专家策略 \( \pi_{\text{expert}} \)**。
- **专家策略来源**：通常来自**人类示范**（如人类玩游戏或执行任务）。
- **过程**：
    1. **数据收集**：使用专家策略与环境交互，采样轨迹 \( \tau = (s_0, a_0, s_1, a_1, \dots, s_T) \)。
        - 初始状态 \( s_0 \sim p_0 \)。
        - 动作由专家策略选择：\( a_t \sim \pi_{\text{expert}}(\cdot|s_t) \)。
        - 状态转移由环境动力学 \( P \) 决定。
    2. **构建数据集**：从轨迹中提取状态-动作对 \( (s, a) \)。
    3. **监督训练**：最小化损失函数 \( \mathcal{L} \)（如交叉熵），使 \( \pi_\theta(a|s) \) 预测专家在状态 \( s \) 下选择的动作 \( a \)。
        \[
        \min_\theta \mathbb{E}_{(s,a) \sim \text{专家数据}} [\mathcal{L}(\pi_\theta(\cdot|s), a)]
        \]
- **本质**：这是一种**监督学习**，不涉及奖励信号。

#### 2.2 与大型语言模型预训练的联系
- **核心洞见**：大型语言模型的**下一词预测（Next Token Prediction）** 可以被视为一种行为克隆。
    - **专家**：互联网文本的人类作者。
    - **任务**：给定一段文本（状态），模型需要预测人类作者实际写下的下一个词（动作）。
- **意义**：这说明了行为克隆可以作为强大模型的**有效起点和初始化方法**（如 AlphaGo 的初始策略、LLM 预训练）。

#### 2.3 优点与局限性
- **优点**：
    - **简单直观**。
    - 当**专家示范数据充足**时非常有效（例如，LLM 预训练数据来自海量人类文本）。
- **局限性与成本**：
    - 获取专家示范可能**昂贵或困难**（例如，需要雇佣专家玩一款新游戏）。
    - 其根本缺陷在于**分布偏移问题**。

---

### 3. 行为克隆的核心问题：分布偏移

#### 3.1 问题描述
- **监督学习中的分布偏移**：模型在训练分布上表现良好，但在测试分布不同时性能下降。
- **行为克隆中的具体表现**：
    - **训练分布**：状态序列由**专家策略** \( \pi_{\text{expert}} \) 的动作所驱动。专家策略通常很优秀，因此访问的**状态空间是“好”状态的子集**。
    - **部署（测试）分布**：状态序列由**被训练的策略** \( \pi_\theta \) 自身的动作所驱动。由于 \( \pi_\theta \) 不完美，它会逐渐偏离专家轨迹，访问到训练时未见过的“坏”状态。
- **结果**：在“坏”状态下，克隆策略**不知道如何恢复**，因为它从未在这些状态下学习过专家的纠正动作。这导致错误累积，性能迅速恶化。

#### 3.2 典型案例：自动驾驶
- **专家司机**：轻微偏离车道时，知道如何微调方向盘回到正轨。
- **行为克隆模型**：如果训练数据中专家总是开得很直，模型只学会了在“直行状态”下操作。一旦车辆因微小误差偏离中心，模型进入一个“偏离状态”，由于从未见过专家在此状态下的恢复动作，它可能做出错误决策，导致事故。

---

### 4. 强化学习的核心特征与范式

#### 4.1 关键洞见：数据生成依赖于策略
- **与监督学习的根本区别**：
    - **监督学习**：假设数据是**静态给定**的，独立于模型。
    - **强化学习**：用于训练的数据**动态生成**，取决于当前与环境交互的**策略本身**。不同的策略产生不同的状态-动作分布。
- **这一特性同时适用于**：基于奖励的RL和模仿学习。

#### 4.2 两种学习范式
基于数据生成策略与当前训练策略的关系，分为两类：

| 范式 | 定义 | 数据来源 | 主要挑战 | 举例 |
|------|------|----------|----------|------|
| **离策略（Off-Policy）学习** | 用于训练的数据由**不同于**当前被训练的策略生成。 | 专家策略、旧版本的策略、随机策略等。 | **分布偏移**：训练数据分布与当前策略产生的分布不匹配。 | **纯行为克隆**（使用固定专家数据）。 |
| **在策略（On-Policy）学习** | 用于训练的数据**必须由当前正在被训练的策略本身**生成。 | 当前策略 \( \pi_\theta \)。 | **样本效率**：每次策略更新后，旧数据即失效，需要重新采样，可能计算量大。 | 许多经典策略梯度方法（如PPO）。 |

- **行为克隆本质上是离策略的**，因此天然受到分布偏移问题的困扰。

---

### 5. 解决方案方向：DAgger 算法

- **名称**：数据集聚合（Dataset Aggregation）。
- **核心思想**：通过迭代过程，使模仿学习**更加“在策略”**，从而缓解分布偏移。
- **步骤**（迭代进行）：
    1. 使用**当前策略** \( \pi_\theta \) 与环境交互，收集轨迹（访问可能包含错误的状态）。
    2. 对于这些轨迹中的每个状态，请**专家提供正确的动作标签**（即“如果在这个状态下，专家会怎么做？”）。
    3. 将新的（状态，专家动作）对**聚合**到数据集中。
    4. 在**整个聚合数据集**上重新训练策略 \( \pi_\theta \)。
- **效果**：随着迭代进行，数据集越来越覆盖当前策略可能访问到的状态空间，使训练分布更接近测试分布，从而**减轻分布偏移**。

---

### 总结要点

1.  **吸收状态**是处理随机终止时间的数学技巧，它将有限时域MDP转换为无限时域问题。
2.  **模仿学习（行为克隆）** 是一种简单有效的监督学习方法，特别适合专家数据丰富的场景（如LLM预训练）。
3.  **分布偏移**是行为克隆的主要瓶颈：克隆策略在部署时会进入训练时未见的“坏状态”，因不知如何恢复而失败。
4.  **强化学习的本质**是**数据依赖于策略**，这引出了**离策略**与**在策略**学习的核心区分。
5.  **DAgger算法**通过迭代式地使用当前策略收集数据并请专家标注，将模仿学习转向更“在策略”的方式，是解决分布偏移问题的一种实用方案。
