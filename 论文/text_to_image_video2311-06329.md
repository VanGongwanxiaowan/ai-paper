https://arxiv.org/pdf/2311.06329

# 《AI文本到图像与文本到视频生成器综述》PDF总结
## 一、研究背景与意义
1. **技术基础**：近年来，深度学习与自然语言处理（NLP）技术飞速发展，催生出AI文本到图像（Text-to-Image）和文本到视频（Text-to-Video）生成器。这类技术借助注意力机制循环神经网络、生成对抗网络（GAN）、Transformer等复杂技术，将文本描述转化为高质量的图像或视频。
2. **核心动机**：旨在实现内容创作流程自动化，以高效、经济的方式快速生成多样化内容，提升用户参与度与体验。
3. **应用领域**
    - **营销**：生成产品设计、产品目录及用户手册。
    - **教育**：制作教学视频与动画，优化学习体验。
    - **娱乐**：创作电影宣传视频、预告片等。


## 二、AI文本到图像生成器（Text-to-Image）
### 1. 核心模型对比
|模型|架构|训练数据|图像质量|计算需求|可解释性|创新性|
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|CogView2|分层Transformer架构|大规模纯文本数据集|高分辨率、高质量|相对高效|难解释|生成速度快、效率高，支持文本引导的交互式图像编辑|
|DALL-E 2|结合StyleGAN2架构的大型Transformer语言模型|大规模图文对数据集|高质量、多样化|高计算成本|易解释|可处理复杂多样的文本提示（如问题、指令），生成更广泛的物体与场景|
|Imagen|大型冻结T5-XXL编码器+扩散模型|大规模纯文本语料库|高质量、照片级真实感|相对高效|易解释|利用现有大型语言模型进行图像生成，提升图像真实感与语言理解深度|

### 2. 关键技术特点
- **CogView2**：先生成低分辨率图像，再通过迭代超分辨率模块（局部并行自回归生成）优化，生成速度比前代CogView快10倍。
- **DALL-E 2**：采用1750亿参数的大型Transformer模型，通过“预训练（纯文本）+微调（图文对）”多阶段训练，以自回归方式生成图像令牌，再经StyleGAN2生成1024x1024高分辨率图像，但自回归生成限制了速度与可扩展性。
- **Imagen**：文本经T5-XXL编码为嵌入向量，由条件扩散模型生成64x64图像，再通过文本条件超分辨率扩散模型将图像分辨率提升至256x256、1024x1024；在COCO数据集的FID分数、图文对齐指标及DrawBench基准测试中表现优异。


## 三、AI文本到视频生成器（Text-to-Video）
### 1. 核心模型对比
|视频生成器|模型类型|优势|局限性|
| ---- | ---- | ---- | ---- |
|Make-A-Video|基于扩散模型的文本到图像（T2I）模型扩展，结合时空分解扩散模型|训练速度快，采用无监督学习（无需配对文本-视频数据），继承图像生成模型能力，支持时空维度超分辨率，生成高清、高帧率视频|无法学习文本与视频中特定现象的关联|
|Imagen Video|级联视频扩散模型（冻结T5文本编码器+基础视频扩散模型+时空超分辨率扩散模型）|高保真度、视频生成多样化，具备3D物体理解能力，可生成文本动画，支持128帧1280x768高清视频（24帧/秒）|训练数据存在问题，易产生社会偏见与刻板印象|
|Phenaki|带Transformer的编码器-解码器模型（C-ViViT架构）|视频预测性能好，可基于文本和起始帧生成长时长、时间连贯的多样化视频，能泛化到视频数据集外的场景|训练数据集存在偏见，仅支持简单动作与运动，缺乏细粒度细节|
|GODIVA|带3D稀疏注意力机制的预训练文本到视频模型|降低计算成本，零样本能力优秀，注意力机制兼顾时空与行列信息|难生成高分辨率长视频，文本到视频生成任务的评估仍具挑战性|
|CogVideo|基于CogView2（文本到图像预训练模型）的大规模预训练模型（94亿参数）|高效利用图像生成能力，通过多帧率分层训练策略提升文本与视频时间维度的对齐度，优化复杂语义运动的生成准确性|输入序列长度受限，模型规模大，对GPU内存要求高|
|NUWA|带3D Transformer编解码器的多模态预训练模型|通过3D邻近注意力（3DNA）机制降低计算复杂度，零样本能力优秀，在8项下游视觉合成任务中表现优异|视频帧中文本与视频对齐效果差|

### 2. 关键技术特点
- **Make-A-Video**：首次实现时空双维度超分辨率，无需文本-视频配对数据，依托文本-图像联合先验实现视频数据规模化扩展。
- **Phenaki**：通过C-ViViT架构将视频压缩为离散嵌入（令牌），利用时间冗余提升重建质量并减少令牌数量；结合预训练语言模型T5X生成的文本嵌入，实现故事性文本序列的视频生成。
- **NUWA**：统一处理语言、图像、视频，支持8项视觉合成任务（含文本到图像/视频生成、视频预测等），无需显式训练数据即可完成文本引导的图像/视频编辑。


## 四、当前技术挑战与未来方向
### 1. 核心挑战
- **数据需求**：需大规模高质量训练数据集，数据获取与标注难度大。
- **可解释性**：生成结果的推理逻辑不明确，难以解释视觉内容的生成依据。
- **内容对齐**：生成内容可能与预期信息/愿景不符，导致错误或冲突，甚至违背社会规范，引发误解或误传。
- **性能平衡**：视觉质量与处理速度存在权衡，高画质图像/视频的计算成本高、生成速度慢，难以快速批量生成内容。
- **模型局限**：文本到图像模型依赖特定数据集，适用领域受限；文本到视频模型难以生成高连贯性视频，且计算资源消耗大、可扩展性差。

### 2. 未来研究方向
- **效率与可及性**：优化模型架构，降低计算成本，提升生成速度，使技术更易普及。
- **领域扩展**：减少对特定数据集的依赖，拓展模型在更多专业领域（如医疗、工业设计）的应用。
- **质量提升**：增强文本与视觉内容的对齐度，提升视频的时间连贯性与细节丰富度，减少偏见与错误。


## 五、结论
AI文本到图像与文本到视频生成器是极具变革性的技术，在营销、教育、娱乐等领域展现出广阔应用前景。当前模型已实现高质量、多样化内容生成的突破，但仍面临数据、可解释性、性能平衡等挑战。未来，随着技术研发的持续推进，更高效、强大、易获取的生成系统将推动数字内容创作方式的革新，成为AI领域持续活跃的研究方向。

**中文译文**：
CogView2 [8] 是一款人工智能文本转图像生成模型，它采用基于分层Transformer的技术方案，可根据文本描述生成对应图像。CogView2 搭载了跨模态通用语言模型（CogLM）——这是一个拥有60亿参数的预训练Transformer模型，该模型通过自监督任务对文本和图像的令牌序列中各类令牌进行掩码与预测。CogView2 的分层结构设计使其能够高效快速地生成高分辨率图像：先生成低分辨率图像，再通过一个采用局部并行自回归生成技术的迭代超分辨率模块对图像进行优化细化。在生成分辨率相近、质量更优的图像时，CogView2 的速度比采用滑动窗口超分辨率技术的初代 CogView [9] 快10倍。此外，CogView2 还支持由文本引导的交互式图像编辑功能。

### 翻译说明
1.  **术语精准性**
    - `text-to-image generator` 译为 **文本转图像生成模型**，契合AI领域的通用表述，避免直译“生成器”的生硬感。
    - `hierarchical transformer-based approach` 译为 **基于分层Transformer的技术方案**，准确体现模型架构特点，“分层”对应 `hierarchical`，“技术方案”使表述更符合中文技术文档用语。
    - `self-supervised task` 译为 **自监督任务**，为机器学习领域的标准译法。
    - `local parallel autoregressive generation` 译为 **局部并行自回归生成**，保留技术核心逻辑，无增删语义。

2.  **句式流畅性**
    - 英文长句拆分为符合中文表达习惯的短句，如将 `The hierarchical design of CogView2 allows for...` 拆分为“CogView2 的分层结构设计使其能够……：先生成……，再通过……优化细化”，逻辑更清晰。
    - 被动语态 `is used` 转为主动表述“采用”，更贴合中文科技文本的行文风格。

3.  **细节补充**
    - `6B parameter` 译为 **60亿参数**，“B”是 `billion` 的缩写，在技术文档中需明确数值量级。
    - `sliding-window superresolution` 译为 **滑动窗口超分辨率技术**，补充“技术”二字，使术语更完整。


**表1. 人工智能文本转图像生成器及其对比**

|生成器|架构|训练数据|图像质量|计算需求|可解释性|创新性|
|----|----|----|----|----|----|----|
|CogView2|分层Transformer架构|大规模纯文本数据集|高分辨率、质量更优|相对高效|难以解释|生成快速高效|
|DALL-E 2|结合StyleGAN2架构的大型Transformer语言模型|大规模图文对数据集|高质量、多样性强|计算成本高|易于解释|可处理复杂多样的文本提示|
|Imagen|大型冻结T5-XXL编码器+扩散模型|大规模纯文本语料库|高质量、照片级真实感|相对高效|易于解释|借助现有语言模型实现图像生成|

### B. Dall-E 2
DALL-E 2 [10] 是另一款最先进的AI文本转图像生成器，由OpenAI在初代DALL-E模型的成功基础上开发而成。DALL-E 2的核心思路是：通过训练一个拥有1750亿参数的大型Transformer模型（这是目前已训练的最大规模语言模型），实现从文本输入生成1024×1024分辨率的高画质图像。

与采用简单VQ-VAE架构生成图像的初代DALL-E不同，DALL-E 2使用了StyleGAN2架构——这是一种更强大的生成模型，能生成更逼真、更多样的图像。此外，DALL-E 2可处理更复杂多样的文本提示（比如问题或指令），并能生成更丰富的物体与场景。

在训练DALL-E 2时，OpenAI收集了大规模的图文对数据集，采用“先在大型文本语料库上预训练，再在图文数据集上微调”的多阶段训练流程。在推理阶段，给定文本提示后，DALL-E 2会以自回归方式生成图像令牌序列，每个令牌对应最终图像的一个区块；最后，这些图像令牌会输入StyleGAN2生成器，得到最终的高分辨率图像。

DALL-E 2在生成与输入文本高度关联的高质量、多样化图像方面表现出色，但它仍受限于自回归生成流程（这会制约其速度与可扩展性），同时在大型图像数据集上训练大规模Transformer模型的计算成本也较高。


### C. Imagen（伊马根）
谷歌的Imagen [11] 是一款AI文本转图像生成模型，它结合了大型Transformer语言模型的优势与扩散模型的特性，能够生成高质量图像。该模型以大型冻结T5-XXL编码器为基础——此编码器会将输入文本编码为嵌入向量，再通过条件扩散模型将文本嵌入向量映射为64×64分辨率的图像。

此外，Imagen还利用文本条件超分辨率扩散模型，将图像分辨率从64×64逐步提升至256×256，再从256×256提升至1024×1024。在COCO数据集上，Imagen在FID分数（衡量生成图像与真实图像相似度的指标）和图文对齐度方面均达到了当前最先进水平；而在面向文本转图像模型的综合挑战性基准测试DrawBench中，通过并列对比可知，Imagen的性能也优于当前主流方法。

Imagen的研究发现，以T5为代表的大型语言模型在为图像合成任务进行文本编码时效率极高，尤其是当这些模型经过纯文本语料库预训练后，这一优势更为明显——这表明现有语言模型具备用于图像生成任务的潜力。借助这一特性，Imagen生成的图像不仅能达到更高的照片级真实感，还能更深度地理解文本语义。

如前文所述，CogView2、DALL-E 2与Imagen这类主流AI文本转图像生成模型，均采用不同技术路径实现从文本输入到图像生成的过程：CogView2基于分层Transformer技术，可快速高效地生成高分辨率图像；DALL-E 2依托大型Transformer语言模型，结合强大的StyleGAN2架构，能生成各类逼真视觉内容；Imagen则融合扩散模型与大型Transformer语言模型的优势，实现高质量图像生成。这三款模型均能生成与输入文本高度相关的多样化、高质量图像，表现十分出色。


### 关键术语补充说明
1. **FID分数（Fréchet Inception Distance）**：衡量生成图像与真实图像分布差异的常用指标，数值越低表示生成图像越接近真实图像，是文本转图像领域的核心评估标准之一。
2. **文本嵌入向量（Text Embedding）**：将自然语言文本转化为的数值向量，使计算机能理解文本语义，是连接文本与视觉生成的关键桥梁。
3. **条件扩散模型（Conditional Diffusion Model）**：一种生成式AI模型，通过在扩散过程中引入文本等条件信息，精准控制生成内容的语义与风格，广泛用于高保真图像生成。

