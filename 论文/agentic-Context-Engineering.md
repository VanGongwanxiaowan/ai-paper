视频围绕斯坦福大学2025年10月挂到Arxiv平台的前沿论文《Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models》展开分享，介绍了上下文工程相关现状、问题及论文提出的解决方案，具体如下：
- **论文背景与上下文工程概念**：分享的论文为斯坦福大学所作，2025年10月刚挂到Arxiv，受关注度较高，核心围绕“context energy（上下文工程）”展开，该技术是优化输入给大语言模型的文本，而非修改模型权重，且当前通过调整prompt、context、memory等输入文本信息优化模型任务处理效果的工作较热门。
- **上下文工程现有优势**：相比传统反推（parent），调整输入文本信息的上下文工程具有可解释性强、能快速整合新知识、无需长期训练、可跨模型共享（在一个模型使用后的文本信息可直接给下一个模型用）以及效率有一定优势等特点。
- **上下文工程现有局限性一**：存在“granted death”问题，即许多prompt优化算法倾向生成简洁文本，且将简洁性视为优势，但这未必符合模型需求，过于简洁的信息可能无法为模型提供足够参考，比如在法律、代码编写等具体场景中，笼统简洁的提示无法提供针对性帮助。
- **上下文工程现有局限性二**：存在“从text的一个class”相关问题，过往不少优化上下文的方法会将所有上下文输入模型，让模型总结或精炼，初期可能有帮助（如模型准确率从无上下文时的63.7逐步提升，上下文token数随训练增加），但到某一步（如第60步）可能出现崩溃，上下文长度骤减（从18000多token降至122token），信息大量丢失，准确率下降。
- **论文提出的解决方案及流程**：针对上述问题，论文提出新方法，不将context与test一起输入模型精炼，而是将context视为table，playbook也提供给模型，让模型每次根据当前轨迹生成一条内容加入playbook（累加形式，非重写整体）；整体流程为：以query和bay book为上下文输入模型，模型分生存者、反思者、generator（生成任务轨迹）三个角色，任务执行后轨迹传给reflector总结经验，再将总结结果传给operator整合成playbook条目，以增量形式添加到playbook。
- **新方法的优势**：一是不会有简洁性倾向，playbook内容不断累加，不会变成简短几句话，避免“granted death”问题；二是每次生成一条内容添加，不会出现整体崩溃现象，解决“从text的一个class”相关崩溃问题。
- **新方法的去重过程**：为避免playbook无限制增长（受大语言模型上下文窗口限制），会进行去重，通过语义embedding比较playbook中不同条目（studies）的相似度，相似度高则去重，简化playbook。
- **新方法的实验相关情况**：实验中，该方法虽输入长公开课类内容，但相比重写整个context的方法，每次输出仅需总结一条内容添加，计算资源花费更少；且因playbook整体固定，结合kv catch等技术，计算资源需求可能低于预期。
- **新方法与过往类似工作的区别**：该方法类似“LEGENIC的memory”工作，但区别在于过往memory工作将记忆存于内存，使用时检索部分内容推断，而该方法的playbook是精简版且整体输入模型作为参考。

<img width="1102" height="685" alt="image" src="https://github.com/user-attachments/assets/4909a588-64a0-4a8c-b3d8-e518acdaa102" />
